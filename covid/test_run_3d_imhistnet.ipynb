{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.1.0+559.g4ce5f80.dirty\n",
      "Python version: 3.7.4 (default, Jul 18 2019, 19:34:02)  [GCC 5.4.0]\n",
      "Numpy version: 1.18.1\n",
      "Pytorch version: 1.5.0\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.3.0\n",
      "Nibabel version: 3.1.0\n",
      "scikit-image version: 0.14.2\n",
      "Pillow version: 7.0.0\n",
      "Tensorboard version: 2.1.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n",
      "----------\n",
      "epoch 1/300\n",
      "1/203, train_loss: 0.6665\n",
      "2/203, train_loss: 188.4254\n",
      "3/203, train_loss: 24.6821\n",
      "4/203, train_loss: 0.0000\n",
      "5/203, train_loss: 116.3344\n",
      "6/203, train_loss: 0.0000\n",
      "7/203, train_loss: 1.5741\n",
      "8/203, train_loss: 6.4486\n",
      "9/203, train_loss: 18.3377\n",
      "10/203, train_loss: 28.4313\n",
      "11/203, train_loss: 59.1707\n",
      "12/203, train_loss: 36.0175\n",
      "13/203, train_loss: 2.7911\n",
      "14/203, train_loss: 29.9327\n",
      "15/203, train_loss: 19.7015\n",
      "16/203, train_loss: 17.6888\n",
      "17/203, train_loss: 29.8526\n",
      "18/203, train_loss: 8.5562\n",
      "19/203, train_loss: 23.3703\n",
      "20/203, train_loss: 32.3583\n",
      "21/203, train_loss: 68.4726\n",
      "22/203, train_loss: 25.9932\n",
      "23/203, train_loss: 31.0501\n",
      "24/203, train_loss: 14.9290\n",
      "25/203, train_loss: 31.9624\n",
      "26/203, train_loss: 8.8446\n",
      "27/203, train_loss: 4.6525\n",
      "28/203, train_loss: 8.6511\n",
      "29/203, train_loss: 9.8641\n",
      "30/203, train_loss: 25.1408\n",
      "31/203, train_loss: 8.4269\n",
      "32/203, train_loss: 1.3624\n",
      "33/203, train_loss: 16.0821\n",
      "34/203, train_loss: 10.1515\n",
      "35/203, train_loss: 17.8335\n",
      "36/203, train_loss: 1.9407\n",
      "37/203, train_loss: 7.3285\n",
      "38/203, train_loss: 38.1202\n",
      "39/203, train_loss: 8.0937\n",
      "40/203, train_loss: 0.0010\n",
      "41/203, train_loss: 3.3747\n",
      "42/203, train_loss: 0.0000\n",
      "43/203, train_loss: 0.0000\n",
      "44/203, train_loss: 8.6274\n",
      "45/203, train_loss: 16.5201\n",
      "46/203, train_loss: 10.7216\n",
      "47/203, train_loss: 1.0790\n",
      "48/203, train_loss: 5.4788\n",
      "49/203, train_loss: 27.3158\n",
      "50/203, train_loss: 8.3457\n",
      "51/203, train_loss: 13.1660\n",
      "52/203, train_loss: 0.0000\n",
      "53/203, train_loss: 0.9808\n",
      "54/203, train_loss: 1.1936\n",
      "55/203, train_loss: 7.0060\n",
      "56/203, train_loss: 2.1868\n",
      "57/203, train_loss: 5.0656\n",
      "58/203, train_loss: 2.0416\n",
      "59/203, train_loss: 0.6954\n",
      "60/203, train_loss: 2.9929\n",
      "61/203, train_loss: 1.6952\n",
      "62/203, train_loss: 4.5342\n",
      "63/203, train_loss: 0.6713\n",
      "64/203, train_loss: 0.5830\n",
      "65/203, train_loss: 2.5273\n",
      "66/203, train_loss: 0.0017\n",
      "67/203, train_loss: 5.5936\n",
      "68/203, train_loss: 2.7430\n",
      "69/203, train_loss: 0.6949\n",
      "70/203, train_loss: 0.6838\n",
      "71/203, train_loss: 1.1648\n",
      "72/203, train_loss: 0.7433\n",
      "73/203, train_loss: 0.7204\n",
      "74/203, train_loss: 1.5697\n",
      "75/203, train_loss: 0.7861\n",
      "76/203, train_loss: 0.8315\n",
      "77/203, train_loss: 1.2351\n",
      "78/203, train_loss: 0.6200\n",
      "79/203, train_loss: 0.7306\n",
      "80/203, train_loss: 0.6907\n",
      "81/203, train_loss: 0.6435\n",
      "82/203, train_loss: 0.9832\n",
      "83/203, train_loss: 0.7034\n",
      "84/203, train_loss: 0.7893\n",
      "85/203, train_loss: 0.6964\n",
      "86/203, train_loss: 0.6917\n",
      "87/203, train_loss: 0.8461\n",
      "88/203, train_loss: 1.0646\n",
      "89/203, train_loss: 0.5637\n",
      "90/203, train_loss: 1.0466\n",
      "91/203, train_loss: 0.6952\n",
      "92/203, train_loss: 1.0849\n",
      "93/203, train_loss: 0.7456\n",
      "94/203, train_loss: 0.7117\n",
      "95/203, train_loss: 0.3048\n",
      "96/203, train_loss: 0.7322\n",
      "97/203, train_loss: 1.7427\n",
      "98/203, train_loss: 1.6458\n",
      "99/203, train_loss: 0.6469\n",
      "100/203, train_loss: 0.5634\n",
      "101/203, train_loss: 0.7752\n",
      "102/203, train_loss: 0.7281\n",
      "103/203, train_loss: 0.8333\n",
      "104/203, train_loss: 0.5649\n",
      "105/203, train_loss: 1.2422\n",
      "106/203, train_loss: 0.8222\n",
      "107/203, train_loss: 0.5865\n",
      "108/203, train_loss: 0.7601\n",
      "109/203, train_loss: 0.7029\n",
      "110/203, train_loss: 0.3841\n",
      "111/203, train_loss: 0.9459\n",
      "112/203, train_loss: 0.1478\n",
      "113/203, train_loss: 0.6792\n",
      "114/203, train_loss: 0.7070\n",
      "115/203, train_loss: 1.2660\n",
      "116/203, train_loss: 0.5826\n",
      "117/203, train_loss: 1.1704\n",
      "118/203, train_loss: 0.5142\n",
      "119/203, train_loss: 1.3354\n",
      "120/203, train_loss: 0.1705\n",
      "121/203, train_loss: 1.0539\n",
      "122/203, train_loss: 1.3646\n",
      "123/203, train_loss: 0.5628\n",
      "124/203, train_loss: 0.0234\n",
      "125/203, train_loss: 3.1680\n",
      "126/203, train_loss: 4.9210\n",
      "127/203, train_loss: 2.0966\n",
      "128/203, train_loss: 0.2334\n",
      "129/203, train_loss: 0.6963\n",
      "130/203, train_loss: 0.5933\n",
      "131/203, train_loss: 0.5903\n",
      "132/203, train_loss: 0.6373\n",
      "133/203, train_loss: 0.7295\n",
      "134/203, train_loss: 0.6570\n",
      "135/203, train_loss: 0.7486\n",
      "136/203, train_loss: 0.5795\n",
      "137/203, train_loss: 1.0870\n",
      "138/203, train_loss: 1.1174\n",
      "139/203, train_loss: 0.5905\n",
      "140/203, train_loss: 0.7128\n",
      "141/203, train_loss: 1.6891\n",
      "142/203, train_loss: 1.2161\n",
      "143/203, train_loss: 1.6873\n",
      "144/203, train_loss: 1.1298\n",
      "145/203, train_loss: 0.0111\n",
      "146/203, train_loss: 1.1230\n",
      "147/203, train_loss: 1.7980\n",
      "148/203, train_loss: 0.8639\n",
      "149/203, train_loss: 0.5676\n",
      "150/203, train_loss: 1.5933\n",
      "151/203, train_loss: 1.6735\n",
      "152/203, train_loss: 0.6702\n",
      "153/203, train_loss: 0.8870\n",
      "154/203, train_loss: 0.7116\n",
      "155/203, train_loss: 0.3047\n",
      "156/203, train_loss: 1.6640\n",
      "157/203, train_loss: 1.2196\n",
      "158/203, train_loss: 0.5955\n",
      "159/203, train_loss: 0.5691\n",
      "160/203, train_loss: 0.7155\n",
      "161/203, train_loss: 0.7338\n",
      "162/203, train_loss: 0.5697\n",
      "163/203, train_loss: 0.6275\n",
      "164/203, train_loss: 1.8005\n",
      "165/203, train_loss: 0.5552\n",
      "166/203, train_loss: 0.7065\n",
      "167/203, train_loss: 0.5934\n",
      "168/203, train_loss: 1.3095\n",
      "169/203, train_loss: 1.2898\n",
      "170/203, train_loss: 0.9788\n",
      "171/203, train_loss: 0.7013\n",
      "172/203, train_loss: 0.9656\n",
      "173/203, train_loss: 0.7870\n",
      "174/203, train_loss: 0.7650\n",
      "175/203, train_loss: 0.8565\n",
      "176/203, train_loss: 0.7079\n",
      "177/203, train_loss: 0.6047\n",
      "178/203, train_loss: 1.0458\n",
      "179/203, train_loss: 0.8486\n",
      "180/203, train_loss: 1.1400\n",
      "181/203, train_loss: 0.5674\n",
      "182/203, train_loss: 0.8883\n",
      "183/203, train_loss: 0.7695\n",
      "184/203, train_loss: 0.6680\n",
      "185/203, train_loss: 0.7238\n",
      "186/203, train_loss: 0.5713\n",
      "187/203, train_loss: 1.3336\n",
      "188/203, train_loss: 0.7536\n",
      "189/203, train_loss: 0.7607\n",
      "190/203, train_loss: 0.5158\n",
      "191/203, train_loss: 0.8757\n",
      "192/203, train_loss: 1.0533\n",
      "193/203, train_loss: 0.6514\n",
      "194/203, train_loss: 0.6040\n",
      "195/203, train_loss: 0.9744\n",
      "196/203, train_loss: 0.3035\n",
      "197/203, train_loss: 0.5745\n",
      "198/203, train_loss: 0.6009\n",
      "199/203, train_loss: 0.6091\n",
      "200/203, train_loss: 0.8375\n",
      "201/203, train_loss: 0.6593\n",
      "202/203, train_loss: 0.6929\n",
      "203/203, train_loss: 0.7308\n",
      "epoch 1 average loss: 6.0977\n",
      "saved new best metric model\n",
      "current epoch: 1 current accuracy: 0.5000 best accuracy: 0.5000 at epoch 1\n",
      "----------\n",
      "epoch 2/300\n",
      "1/203, train_loss: 0.7726\n",
      "2/203, train_loss: 0.7050\n",
      "3/203, train_loss: 0.7795\n",
      "4/203, train_loss: 0.7051\n",
      "5/203, train_loss: 0.7406\n",
      "6/203, train_loss: 0.5537\n",
      "7/203, train_loss: 0.5724\n",
      "8/203, train_loss: 1.9463\n",
      "9/203, train_loss: 1.0852\n",
      "10/203, train_loss: 0.6301\n",
      "11/203, train_loss: 0.6937\n",
      "12/203, train_loss: 0.6984\n",
      "13/203, train_loss: 0.6974\n",
      "14/203, train_loss: 0.6782\n",
      "15/203, train_loss: 0.8129\n",
      "16/203, train_loss: 0.4221\n",
      "17/203, train_loss: 0.8512\n",
      "18/203, train_loss: 0.9324\n",
      "19/203, train_loss: 0.5695\n",
      "20/203, train_loss: 1.5050\n",
      "21/203, train_loss: 0.6255\n",
      "22/203, train_loss: 0.6972\n",
      "23/203, train_loss: 0.7387\n",
      "24/203, train_loss: 0.9576\n",
      "25/203, train_loss: 0.5798\n",
      "26/203, train_loss: 0.7450\n",
      "27/203, train_loss: 0.8652\n",
      "28/203, train_loss: 0.7705\n",
      "29/203, train_loss: 0.6870\n",
      "30/203, train_loss: 0.9155\n",
      "31/203, train_loss: 0.6043\n",
      "32/203, train_loss: 0.5983\n",
      "33/203, train_loss: 0.8967\n",
      "34/203, train_loss: 0.6060\n",
      "35/203, train_loss: 0.7074\n",
      "36/203, train_loss: 0.7005\n",
      "37/203, train_loss: 0.6938\n",
      "38/203, train_loss: 0.6458\n",
      "39/203, train_loss: 0.8886\n",
      "40/203, train_loss: 0.5949\n",
      "41/203, train_loss: 0.5904\n",
      "42/203, train_loss: 0.7603\n",
      "43/203, train_loss: 0.5748\n",
      "44/203, train_loss: 0.7641\n",
      "45/203, train_loss: 0.7399\n",
      "46/203, train_loss: 0.6230\n",
      "47/203, train_loss: 0.7607\n",
      "48/203, train_loss: 0.6932\n",
      "49/203, train_loss: 0.9386\n",
      "50/203, train_loss: 0.6614\n",
      "51/203, train_loss: 0.6771\n",
      "52/203, train_loss: 0.7409\n",
      "53/203, train_loss: 0.6926\n",
      "54/203, train_loss: 0.6913\n",
      "55/203, train_loss: 0.6954\n",
      "56/203, train_loss: 0.6960\n",
      "57/203, train_loss: 0.6967\n",
      "58/203, train_loss: 0.6529\n",
      "59/203, train_loss: 0.8297\n",
      "60/203, train_loss: 0.6916\n",
      "61/203, train_loss: 0.7027\n",
      "62/203, train_loss: 0.6859\n",
      "63/203, train_loss: 0.6600\n",
      "64/203, train_loss: 0.6329\n",
      "65/203, train_loss: 0.6105\n",
      "66/203, train_loss: 0.5886\n",
      "67/203, train_loss: 0.5755\n",
      "68/203, train_loss: 0.3370\n",
      "69/203, train_loss: 0.5643\n",
      "70/203, train_loss: 1.8201\n",
      "71/203, train_loss: 0.8308\n",
      "72/203, train_loss: 0.7062\n",
      "73/203, train_loss: 0.6394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/203, train_loss: 0.5667\n",
      "75/203, train_loss: 0.9009\n",
      "76/203, train_loss: 0.9247\n",
      "77/203, train_loss: 0.8208\n",
      "78/203, train_loss: 0.6975\n",
      "79/203, train_loss: 0.9158\n",
      "80/203, train_loss: 1.2299\n",
      "81/203, train_loss: 0.6946\n",
      "82/203, train_loss: 0.4059\n",
      "83/203, train_loss: 1.0242\n",
      "84/203, train_loss: 2.1976\n",
      "85/203, train_loss: 1.3566\n",
      "86/203, train_loss: 0.6852\n",
      "87/203, train_loss: 0.7132\n",
      "88/203, train_loss: 0.7124\n",
      "89/203, train_loss: 0.8103\n",
      "90/203, train_loss: 2.1112\n",
      "91/203, train_loss: 0.5861\n",
      "92/203, train_loss: 0.5248\n",
      "93/203, train_loss: 2.2310\n",
      "94/203, train_loss: 1.3178\n",
      "95/203, train_loss: 0.9234\n",
      "96/203, train_loss: 1.5154\n",
      "97/203, train_loss: 1.2297\n",
      "98/203, train_loss: 0.5888\n",
      "99/203, train_loss: 1.1340\n",
      "100/203, train_loss: 0.7819\n",
      "101/203, train_loss: 0.5430\n",
      "102/203, train_loss: 0.4156\n",
      "103/203, train_loss: 1.1962\n",
      "104/203, train_loss: 0.7417\n",
      "105/203, train_loss: 0.8430\n",
      "106/203, train_loss: 1.1954\n",
      "107/203, train_loss: 1.8007\n",
      "108/203, train_loss: 0.7656\n",
      "109/203, train_loss: 1.2870\n",
      "110/203, train_loss: 0.7949\n",
      "111/203, train_loss: 0.7257\n",
      "112/203, train_loss: 0.7108\n",
      "113/203, train_loss: 0.7292\n",
      "114/203, train_loss: 0.7535\n",
      "115/203, train_loss: 0.6713\n",
      "116/203, train_loss: 0.6964\n",
      "117/203, train_loss: 0.8683\n",
      "118/203, train_loss: 0.7856\n",
      "119/203, train_loss: 0.7250\n",
      "120/203, train_loss: 0.8533\n",
      "121/203, train_loss: 0.6959\n",
      "122/203, train_loss: 0.7886\n",
      "123/203, train_loss: 0.6936\n",
      "124/203, train_loss: 0.6584\n",
      "125/203, train_loss: 0.7194\n",
      "126/203, train_loss: 0.8894\n",
      "127/203, train_loss: 0.7145\n",
      "128/203, train_loss: 0.6995\n",
      "129/203, train_loss: 0.6945\n",
      "130/203, train_loss: 0.7264\n",
      "131/203, train_loss: 0.6948\n",
      "132/203, train_loss: 0.6973\n",
      "133/203, train_loss: 0.6964\n",
      "134/203, train_loss: 0.7495\n",
      "135/203, train_loss: 0.6490\n",
      "136/203, train_loss: 0.4937\n",
      "137/203, train_loss: 0.8185\n",
      "138/203, train_loss: 1.2814\n",
      "139/203, train_loss: 0.5643\n",
      "140/203, train_loss: 1.0992\n",
      "141/203, train_loss: 0.5850\n",
      "142/203, train_loss: 0.6445\n",
      "143/203, train_loss: 0.6941\n",
      "144/203, train_loss: 0.6900\n",
      "145/203, train_loss: 0.6892\n",
      "146/203, train_loss: 0.7298\n",
      "147/203, train_loss: 0.6864\n",
      "148/203, train_loss: 0.6847\n",
      "149/203, train_loss: 0.7443\n",
      "150/203, train_loss: 0.7480\n",
      "151/203, train_loss: 0.6681\n",
      "152/203, train_loss: 0.6930\n",
      "153/203, train_loss: 0.6662\n",
      "154/203, train_loss: 0.6552\n",
      "155/203, train_loss: 0.7758\n",
      "156/203, train_loss: 0.6299\n",
      "157/203, train_loss: 0.7978\n",
      "158/203, train_loss: 0.7812\n",
      "159/203, train_loss: 0.6527\n",
      "160/203, train_loss: 0.6657\n",
      "161/203, train_loss: 0.6941\n",
      "162/203, train_loss: 0.7575\n",
      "163/203, train_loss: 0.6877\n",
      "164/203, train_loss: 0.6964\n",
      "165/203, train_loss: 0.7106\n",
      "166/203, train_loss: 0.7243\n",
      "167/203, train_loss: 0.8674\n",
      "168/203, train_loss: 0.8186\n",
      "169/203, train_loss: 0.6130\n",
      "170/203, train_loss: 0.6690\n",
      "171/203, train_loss: 0.7127\n",
      "172/203, train_loss: 0.6875\n",
      "173/203, train_loss: 0.6933\n",
      "174/203, train_loss: 0.6894\n",
      "175/203, train_loss: 0.6942\n",
      "176/203, train_loss: 0.7209\n",
      "177/203, train_loss: 0.7098\n",
      "178/203, train_loss: 0.6933\n",
      "179/203, train_loss: 0.6896\n",
      "180/203, train_loss: 0.6644\n",
      "181/203, train_loss: 0.6970\n",
      "182/203, train_loss: 0.6989\n",
      "183/203, train_loss: 0.5784\n",
      "184/203, train_loss: 0.7037\n",
      "185/203, train_loss: 0.5204\n",
      "186/203, train_loss: 0.8398\n",
      "187/203, train_loss: 0.8511\n",
      "188/203, train_loss: 0.6032\n",
      "189/203, train_loss: 0.6061\n",
      "190/203, train_loss: 0.8293\n",
      "191/203, train_loss: 0.6143\n",
      "192/203, train_loss: 0.7094\n",
      "193/203, train_loss: 0.7061\n",
      "194/203, train_loss: 0.7753\n",
      "195/203, train_loss: 0.6967\n",
      "196/203, train_loss: 0.6651\n",
      "197/203, train_loss: 0.6955\n",
      "198/203, train_loss: 0.6716\n",
      "199/203, train_loss: 0.7218\n",
      "200/203, train_loss: 0.6949\n",
      "201/203, train_loss: 0.6956\n",
      "202/203, train_loss: 0.6949\n",
      "203/203, train_loss: 0.6946\n",
      "epoch 2 average loss: 0.7791\n",
      "current epoch: 2 current accuracy: 0.5000 best accuracy: 0.5000 at epoch 1\n",
      "----------\n",
      "epoch 3/300\n",
      "1/203, train_loss: 0.7247\n",
      "2/203, train_loss: 0.6779\n",
      "3/203, train_loss: 0.6971\n",
      "4/203, train_loss: 0.7099\n",
      "5/203, train_loss: 0.6885\n",
      "6/203, train_loss: 0.6976\n",
      "7/203, train_loss: 0.6888\n",
      "8/203, train_loss: 0.7627\n",
      "9/203, train_loss: 0.6933\n",
      "10/203, train_loss: 0.7110\n",
      "11/203, train_loss: 0.6655\n",
      "12/203, train_loss: 0.6990\n",
      "13/203, train_loss: 0.6291\n",
      "14/203, train_loss: 0.6221\n",
      "15/203, train_loss: 0.7109\n",
      "16/203, train_loss: 0.7145\n",
      "17/203, train_loss: 0.7866\n",
      "18/203, train_loss: 0.7271\n",
      "19/203, train_loss: 0.6982\n",
      "20/203, train_loss: 0.7221\n",
      "21/203, train_loss: 1.1416\n",
      "22/203, train_loss: 0.7087\n",
      "23/203, train_loss: 0.6927\n",
      "24/203, train_loss: 0.6976\n",
      "25/203, train_loss: 0.6272\n",
      "26/203, train_loss: 0.8115\n",
      "27/203, train_loss: 0.6160\n",
      "28/203, train_loss: 0.7084\n",
      "29/203, train_loss: 0.7739\n",
      "30/203, train_loss: 0.7206\n",
      "31/203, train_loss: 0.6191\n",
      "32/203, train_loss: 1.1749\n",
      "33/203, train_loss: 0.5619\n",
      "34/203, train_loss: 0.9459\n",
      "35/203, train_loss: 0.6058\n",
      "36/203, train_loss: 0.6388\n",
      "37/203, train_loss: 0.6979\n",
      "38/203, train_loss: 0.6939\n",
      "39/203, train_loss: 0.6806\n",
      "40/203, train_loss: 0.6692\n",
      "41/203, train_loss: 0.6961\n",
      "42/203, train_loss: 0.7807\n",
      "43/203, train_loss: 0.6901\n",
      "44/203, train_loss: 0.7178\n",
      "45/203, train_loss: 0.6737\n",
      "46/203, train_loss: 0.7010\n",
      "47/203, train_loss: 0.7999\n",
      "48/203, train_loss: 0.8063\n",
      "49/203, train_loss: 0.7106\n",
      "50/203, train_loss: 0.7891\n",
      "51/203, train_loss: 0.5647\n",
      "52/203, train_loss: 0.7007\n",
      "53/203, train_loss: 0.6421\n",
      "54/203, train_loss: 0.6966\n",
      "55/203, train_loss: 0.6979\n",
      "56/203, train_loss: 0.6472\n",
      "57/203, train_loss: 0.7515\n",
      "58/203, train_loss: 0.7449\n",
      "59/203, train_loss: 0.6926\n",
      "60/203, train_loss: 0.6922\n",
      "61/203, train_loss: 0.6929\n",
      "62/203, train_loss: 0.6915\n",
      "63/203, train_loss: 0.6441\n",
      "64/203, train_loss: 0.7110\n",
      "65/203, train_loss: 0.8385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.7.4/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6e9b9a701415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6e9b9a701415>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mepoch_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}, train_loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.data import NiftiDataset\n",
    "from monai.transforms import Compose, SpatialPad, AddChannel, ScaleIntensity, Resize, RandRotate90, RandRotate, RandZoom, ToTensor\n",
    "import os\n",
    "\n",
    "from monai.networks.layers.factories import Conv, Dropout, Pool, Norm\n",
    "\n",
    "'''\n",
    "class imhistnet_3d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bins=32,\n",
    "                 no_classes=2,\n",
    "                 pool_kernel=32,\n",
    "                 pool_stride=32):\n",
    "        super(imhistnet_3d, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(1, bins, 1, 1)\n",
    "        self.conv2 = nn.Conv3d(bins, bins, 1, 1, groups=bins)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool3d(pool_kernel, pool_stride)\n",
    "        self.fc1 = nn.Linear(bins*4*4*4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, no_classes)\n",
    "\n",
    "        initialize_params(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.abs(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def initialize_params(module):\n",
    "\n",
    "    for name, m in module.named_modules():\n",
    "        if isinstance(m, nn.Conv3d) and name=='conv1':\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            nn.init.xavier_normal_(m.bias)\n",
    "\n",
    "        elif isinstance(m, nn.Conv3d) and name=='conv2':\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 1.0)\n",
    "\n",
    "        else:\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.xavier_normal_(m.bias)\n",
    "'''\n",
    "\n",
    "class imhistnet_3d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bins=16,\n",
    "                 no_classes=2,\n",
    "                 pool_kernel=32,\n",
    "                 pool_stride=32):\n",
    "        super(imhistnet_3d, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(1, bins, 1, 1)\n",
    "        nn.init.constant_(self.conv1.weight, 1.0)\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv3d(bins, bins, 1, 1, groups=bins)\n",
    "        nn.init.constant_(self.conv2.bias, 1.0)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool3d(pool_kernel, pool_stride)\n",
    "        self.fc1 = nn.Linear(bins*4*4*4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, no_classes)\n",
    "\n",
    "        #initialize_params(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.abs(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    monai.config.print_config()\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Training data paths\n",
    "    data_dir = '/home/marafath/scratch/iran_organized_data2'\n",
    "\n",
    "    covid_pat = 0\n",
    "    non_covid_pat = 0\n",
    "\n",
    "    images_p = []\n",
    "    labels_p = []\n",
    "    images_n = []\n",
    "    labels_n = []\n",
    "\n",
    "    for patient in os.listdir(data_dir):\n",
    "        if int(patient[-1]) == 0 and non_covid_pat > 236:\n",
    "            continue \n",
    "\n",
    "        if int(patient[-1]) == 1:\n",
    "            covid_pat += 1\n",
    "            for series in os.listdir(os.path.join(data_dir,patient)):\n",
    "                labels_p.append(1)\n",
    "                images_p.append(os.path.join(data_dir,patient,series,'cropped_and_resized_image.nii.gz'))\n",
    "        else:\n",
    "            non_covid_pat += 1\n",
    "            for series in os.listdir(os.path.join(data_dir,patient)):\n",
    "                labels_n.append(0)\n",
    "                images_n.append(os.path.join(data_dir,patient,series,'cropped_and_resized_image.nii.gz'))\n",
    "            \n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "\n",
    "    val_images = []\n",
    "    val_labels = []\n",
    "\n",
    "    for i in range(0,len(images_p)):\n",
    "        if i < 407:\n",
    "            train_images.append(images_p[i])\n",
    "            train_labels.append(labels_p[i])\n",
    "        else:\n",
    "            val_images.append(images_p[i])\n",
    "            val_labels.append(labels_p[i])\n",
    "\n",
    "    for i in range(0,len(images_n)):\n",
    "        if i < 405:\n",
    "            train_images.append(images_n[i])\n",
    "            train_labels.append(labels_n[i])\n",
    "        else:\n",
    "            val_images.append(images_n[i])\n",
    "            val_labels.append(labels_n[i])  \n",
    "    \n",
    "    train_labels = np.asarray(train_labels,np.int64)\n",
    "    val_labels = np.asarray(val_labels,np.int64)\n",
    "\n",
    "\n",
    "    # Define transforms\n",
    "    train_transforms = Compose([\n",
    "        ScaleIntensity(),\n",
    "        AddChannel(),\n",
    "        RandRotate(range_x=10.0, range_y=10.0, range_z=10.0, prob=0.5),\n",
    "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "        #SpatialPad((128, 128, 92), mode='constant'),\n",
    "        #Resize((128, 128, 92)),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    \n",
    "    val_transforms = Compose([\n",
    "        ScaleIntensity(),\n",
    "        AddChannel(),\n",
    "        #SpatialPad((128, 128, 92), mode='constant'),\n",
    "        #Resize((128, 128, 92)),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    # create a training data loader\n",
    "    train_ds = NiftiDataset(image_files=train_images, labels=train_labels, transform=train_transforms)\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "    # create a validation data loader\n",
    "    val_ds = NiftiDataset(image_files=val_images, labels=val_labels, transform=val_transforms)\n",
    "    val_loader = DataLoader(val_ds, batch_size=2, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    model = imhistnet_3d().to(device)\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in ['conv1.weight', 'conv2.bias']:\n",
    "            param.required_grad = False\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-2)\n",
    "    \n",
    "    # finetuning\n",
    "    #model.load_state_dict(torch.load('best_metric_model_d121.pth'))\n",
    "\n",
    "    # start a typical PyTorch training\n",
    "    val_interval = 1\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = list()\n",
    "    metric_values = list()\n",
    "    writer = SummaryWriter()\n",
    "    epc = 300 # Number of epoch\n",
    "    for epoch in range(epc):\n",
    "        print('-' * 10)\n",
    "        print('epoch {}/{}'.format(epoch + 1, epc))\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = batch_data[0].to(device), batch_data[1].to(device=device, dtype=torch.int64)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_len = len(train_ds) // train_loader.batch_size\n",
    "            print('{}/{}, train_loss: {:.4f}'.format(step, epoch_len, loss.item()))\n",
    "            writer.add_scalar('train_loss', loss.item(), epoch_len * epoch + step)\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print('epoch {} average loss: {:.4f}'.format(epoch + 1, epoch_loss))\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                num_correct = 0.\n",
    "                metric_count = 0\n",
    "                for val_data in val_loader:\n",
    "                    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "                    val_outputs = model(val_images)\n",
    "                    value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n",
    "                    metric_count += len(value)\n",
    "                    num_correct += value.sum().item()\n",
    "                metric = num_correct / metric_count\n",
    "                metric_values.append(metric)\n",
    "                #torch.save(model.state_dict(), 'model_d121_epoch_{}.pth'.format(epoch + 1))\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), '/home/marafath/scratch/saved_models/best_metric_model_imhistnet_no_avgpool.pth')\n",
    "                    print('saved new best metric model')\n",
    "                print('current epoch: {} current accuracy: {:.4f} best accuracy: {:.4f} at epoch {}'.format(\n",
    "                    epoch + 1, metric, best_metric, best_metric_epoch))\n",
    "                writer.add_scalar('val_accuracy', metric, epoch + 1)\n",
    "    print('train completed, best_metric: {:.4f} at epoch: {}'.format(best_metric, best_metric_epoch))\n",
    "    writer.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
