{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.data import NiftiDataset, CSVSaver\n",
    "from monai.transforms import Compose, SpatialPad, AddChannel, ScaleIntensity, Resize, RandRotate90, RandRotate, RandZoom, ToTensor\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from monai.networks.layers.factories import Conv, Dropout, Pool, Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, spatial_dims: int, in_channels: int, growth_rate: int, bn_size: int, dropout_prob: float\n",
    "    ) -> None:\n",
    "        super(_DenseLayer, self).__init__()\n",
    "\n",
    "        out_channels = bn_size * growth_rate\n",
    "        conv_type: Callable = Conv[Conv.CONV, spatial_dims]\n",
    "        norm_type: Callable = Norm[Norm.BATCH, spatial_dims]\n",
    "        dropout_type: Callable = Dropout[Dropout.DROPOUT, spatial_dims]\n",
    "\n",
    "        self.add_module(\"norm1\", norm_type(in_channels))\n",
    "        self.add_module(\"relu1\", nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv1\", conv_type(in_channels, out_channels, kernel_size=1, bias=False))\n",
    "\n",
    "        self.add_module(\"norm2\", norm_type(out_channels))\n",
    "        self.add_module(\"relu2\", nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv2\", conv_type(out_channels, growth_rate, kernel_size=3, padding=1, bias=False))\n",
    "\n",
    "        if dropout_prob > 0:\n",
    "            self.add_module(\"dropout\", dropout_type(dropout_prob))\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, spatial_dims: int, layers: int, in_channels: int, bn_size: int, growth_rate: int, dropout_prob: float\n",
    "    ) -> None:\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(layers):\n",
    "            layer = _DenseLayer(spatial_dims, in_channels, growth_rate, bn_size, dropout_prob)\n",
    "            in_channels += growth_rate\n",
    "            self.add_module(\"denselayer%d\" % (i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int) -> None:\n",
    "        super(_Transition, self).__init__()\n",
    "\n",
    "        conv_type: Callable = Conv[Conv.CONV, spatial_dims]\n",
    "        norm_type: Callable = Norm[Norm.BATCH, spatial_dims]\n",
    "        pool_type: Callable = Pool[Pool.AVG, spatial_dims]\n",
    "\n",
    "        self.add_module(\"norm\", norm_type(in_channels))\n",
    "        self.add_module(\"relu\", nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv\", conv_type(in_channels, out_channels, kernel_size=1, bias=False))\n",
    "        self.add_module(\"pool\", pool_type(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Densenet based on: \"Densely Connected Convolutional Networks\" https://arxiv.org/pdf/1608.06993.pdf\n",
    "    Adapted from PyTorch Hub 2D version:\n",
    "    https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\n",
    "\n",
    "    Args:\n",
    "        spatial_dims: number of spatial dimensions of the input image.\n",
    "        in_channels: number of the input channel.\n",
    "        out_channels: number of the output classes.\n",
    "        init_features: number of filters in the first convolution layer.\n",
    "        growth_rate: how many filters to add each layer (k in paper).\n",
    "        block_config: how many layers in each pooling block.\n",
    "        bn_size: multiplicative factor for number of bottle neck layers.\n",
    "                      (i.e. bn_size * k features in the bottleneck layer)\n",
    "        dropout_prob: dropout rate after each dense layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        init_features: int = 64,\n",
    "        growth_rate: int = 32,\n",
    "        block_config: Sequence[int] = (6, 12, 24, 16),\n",
    "        bn_size: int = 4,\n",
    "        dropout_prob: float = 0.0,\n",
    "        bins=8,\n",
    "        pool_kernel=32,\n",
    "        pool_stride=32\n",
    "    ) -> None:\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        conv_type: Callable = Conv[Conv.CONV, spatial_dims]\n",
    "        norm_type: Callable = Norm[Norm.BATCH, spatial_dims]\n",
    "        pool_type: Callable = Pool[Pool.MAX, spatial_dims]\n",
    "        avg_pool_type: Callable = Pool[Pool.ADAPTIVEAVG, spatial_dims]\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"conv0\", conv_type(in_channels, init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "                    (\"norm0\", norm_type(init_features)),\n",
    "                    (\"relu0\", nn.ReLU(inplace=True)),\n",
    "                    (\"pool0\", pool_type(kernel_size=3, stride=2, padding=1)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        in_channels = init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                spatial_dims=spatial_dims,\n",
    "                layers=num_layers,\n",
    "                in_channels=in_channels,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                dropout_prob=dropout_prob,\n",
    "            )\n",
    "            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n",
    "            in_channels += num_layers * growth_rate\n",
    "            if i == len(block_config) - 1:\n",
    "                self.features.add_module(\"norm5\", norm_type(in_channels))\n",
    "            else:\n",
    "                _out_channels = in_channels // 2\n",
    "                trans = _Transition(spatial_dims, in_channels=in_channels, out_channels=_out_channels)\n",
    "                self.features.add_module(\"transition%d\" % (i + 1), trans)\n",
    "                in_channels = _out_channels\n",
    "\n",
    "        # pooling and classification\n",
    "        '''\n",
    "        self.class_layers = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"relu\", nn.ReLU(inplace=True)),\n",
    "                    (\"norm\", avg_pool_type(1)),\n",
    "                    (\"flatten\", nn.Flatten(1)), \n",
    "                    (\"class\", nn.Linear(in_channels, out_channels)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc1 = nn.Linear(1024*4*4*4, 1024)\n",
    "        self.fc2 = nn.Linear(2048, out_channels)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, conv_type):  # type: ignore\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, norm_type):  # type: ignore\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        # 3D-ImHistNet\n",
    "        self.conv1 = nn.Conv3d(1, bins, 1, 1)\n",
    "        nn.init.constant_(self.conv1.weight, 1.0)\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv3d(bins, bins, 1, 1, groups=bins)\n",
    "        nn.init.constant_(self.conv2.bias, 1.0)\n",
    "    \n",
    "        self.avgpool = nn.AvgPool3d(pool_kernel, pool_stride)\n",
    "        self.hist_fc = nn.Linear(bins*4*4*4, 1024)\n",
    "        #self.fc2 = nn.Linear(1024, no_classes)\n",
    "\n",
    "        #initialize_params(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.features(x)\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "        x1 = self.fc1(x1)\n",
    "        \n",
    "        x2 = self.conv1(x)\n",
    "        x2 = torch.abs(x2)\n",
    "        x2 = self.conv2(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x2 = self.avgpool(x2)\n",
    "        x2 = torch.flatten(x2, 1)\n",
    "        x2 = self.hist_fc(x2)\n",
    "        \n",
    "        x_cat = torch.cat([x1, x2], 1)\n",
    "        x_cat = self.fc2(x_cat)\n",
    "        return x_cat\n",
    "\n",
    "\n",
    "def densenet121(**kwargs) -> DenseNet:\n",
    "    model = DenseNet(init_features=64, growth_rate=32, block_config=(6, 12, 24, 16), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def densenet169(**kwargs) -> DenseNet:\n",
    "    model = DenseNet(init_features=64, growth_rate=32, block_config=(6, 12, 32, 32), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def densenet201(**kwargs) -> DenseNet:\n",
    "    model = DenseNet(init_features=64, growth_rate=32, block_config=(6, 12, 48, 32), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def densenet264(**kwargs) -> DenseNet:\n",
    "    model = DenseNet(init_features=64, growth_rate=32, block_config=(6, 12, 64, 48), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.1.0+560.gc89bf24.dirty\n",
      "Python version: 3.7.4 (default, Jul 18 2019, 19:34:02)  [GCC 5.4.0]\n",
      "Numpy version: 1.18.1\n",
      "Pytorch version: 1.5.0\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.3.0\n",
      "Nibabel version: 3.1.0\n",
      "scikit-image version: 0.14.2\n",
      "Pillow version: 7.0.0\n",
      "Tensorboard version: 2.1.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n",
      "evaluation metric: 0.765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No-COVID     0.8193    0.6800    0.7432       100\n",
      "       COVID     0.7265    0.8500    0.7834       100\n",
      "\n",
      "    accuracy                         0.7650       200\n",
      "   macro avg     0.7729    0.7650    0.7633       200\n",
      "weighted avg     0.7729    0.7650    0.7633       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    monai.config.print_config()\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # IXI dataset as a demo, downloadable from https://brain-development.org/ixi-dataset/\n",
    "    \n",
    "    # Validation data paths\n",
    "    data_dir = '/home/marafath/scratch/iran_organized_data2'\n",
    "    class_names = ['No-COVID', 'COVID']\n",
    "    \n",
    "    covid_pat = 0\n",
    "    non_covid_pat = 0\n",
    "\n",
    "    images_p = []\n",
    "    labels_p = []\n",
    "    images_n = []\n",
    "    labels_n = []\n",
    "\n",
    "    for patient in os.listdir(data_dir):\n",
    "        if int(patient[-1]) == 0 and non_covid_pat > 236:\n",
    "            continue \n",
    "\n",
    "        if int(patient[-1]) == 1:\n",
    "            covid_pat += 1\n",
    "            for series in os.listdir(os.path.join(data_dir,patient)):\n",
    "                labels_p.append(1)\n",
    "                images_p.append(os.path.join(data_dir,patient,series,'cropped_and_resized_image.nii.gz'))\n",
    "        else:\n",
    "            non_covid_pat += 1\n",
    "            for series in os.listdir(os.path.join(data_dir,patient)):\n",
    "                labels_n.append(0)\n",
    "                images_n.append(os.path.join(data_dir,patient,series,'cropped_and_resized_image.nii.gz'))\n",
    "            \n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "\n",
    "    val_images = []\n",
    "    val_labels = []\n",
    "    \n",
    "    for i in range(0,len(images_p)):\n",
    "        if i < 407:\n",
    "            train_images.append(images_p[i])\n",
    "            train_labels.append(labels_p[i])\n",
    "        else:\n",
    "            val_images.append(images_p[i])\n",
    "            val_labels.append(labels_p[i])\n",
    "\n",
    "    for i in range(0,len(images_n)):\n",
    "        if i < 405:\n",
    "            train_images.append(images_n[i])\n",
    "            train_labels.append(labels_n[i])\n",
    "        else:\n",
    "            val_images.append(images_n[i])\n",
    "            val_labels.append(labels_n[i])  \n",
    "    \n",
    "    train_labels = np.asarray(train_labels,np.int64)\n",
    "    val_labels = np.asarray(val_labels,np.int64)\n",
    "    \n",
    "    test_images = val_images\n",
    "    test_labels = val_labels\n",
    "\n",
    "    # Define transforms for image\n",
    "    val_transforms = Compose([\n",
    "        ScaleIntensity(),\n",
    "        AddChannel(),\n",
    "        #SpatialPad((128, 128, 92), mode='constant'),\n",
    "        #Resize((128, 128, 92)),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Define nifti dataset\n",
    "    val_ds = NiftiDataset(image_files=test_images, labels=test_labels, transform=val_transforms, image_only=False)\n",
    "    val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())\n",
    "    \n",
    "    # Create DenseNet121\n",
    "    device = torch.device('cuda:0')\n",
    "    model = densenet121(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=2,\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load('/home/marafath/scratch/saved_models/best_model_densenet_imhistnet.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = list()\n",
    "    y_pred = list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        num_correct = 0.\n",
    "        metric_count = 0\n",
    "        saver = CSVSaver(output_dir='./output')\n",
    "        for val_data in val_loader:\n",
    "            val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "            val_outputs = model(val_images).argmax(dim=1)\n",
    "            value = torch.eq(val_outputs, val_labels)\n",
    "            metric_count += len(value)\n",
    "            num_correct += value.sum().item()\n",
    "            saver.save_batch(val_outputs, val_data[2])\n",
    "            \n",
    "            for i in range(len(val_outputs)):\n",
    "                y_true.append(val_labels[i].item())\n",
    "                y_pred.append(val_outputs[i].item())\n",
    "            \n",
    "        metric = num_correct / metric_count\n",
    "        print('evaluation metric:', metric)\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "        saver.finalize()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of rows: 200\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "filename = './output/predictions.csv'\n",
    "rows = [] \n",
    "\n",
    "with open(filename, 'r') as csvfile: \n",
    "    # creating a csv reader object \n",
    "    csvreader = csv.reader(csvfile) \n",
    "  \n",
    "    # extracting each data row one by one \n",
    "    for row in csvreader: \n",
    "        rows.append(row) \n",
    "  \n",
    "    # get total number of rows \n",
    "    print(\"Total no. of rows: %d\"%(csvreader.line_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1934813 1.0\n"
     ]
    }
   ],
   "source": [
    "rw = rows[0][0]\n",
    "pat = rw[44:51]\n",
    "lab = float(rw[52:53])\n",
    "\n",
    "print(pat, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "84\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "unique_pat = []\n",
    "pat_label_gt = []\n",
    "pat_label_pr = []\n",
    "\n",
    "rw = rows[0][0]\n",
    "gt = rows[i][1]\n",
    "unique_pat.append(rw[44:51])\n",
    "pat_label_pr.append(int(rw[52:53]))\n",
    "pat_label_gt.append(int(gt[0]))\n",
    "\n",
    "cnt = 0\n",
    "for i in range(1,len(rows)):\n",
    "    rw = rows[i][0]\n",
    "    pat = rw[44:51]\n",
    "    lab = int(rw[52:53])\n",
    "    gt = rows[i][1]\n",
    "    for j in range(cnt,len(unique_pat)):\n",
    "        if pat == unique_pat[j]:\n",
    "            break\n",
    "        else:\n",
    "            unique_pat.append(pat)\n",
    "            pat_label_gt.append(int(gt[0]))\n",
    "            pat_label_pr.append(lab)\n",
    "            cnt += 1\n",
    "\n",
    "print(len(unique_pat)) \n",
    "print(len(pat_label_gt))\n",
    "print(len(pat_label_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No-COVID     0.6410    0.7576    0.6944        33\n",
      "       COVID     0.8222    0.7255    0.7708        51\n",
      "\n",
      "    accuracy                         0.7381        84\n",
      "   macro avg     0.7316    0.7415    0.7326        84\n",
      "weighted avg     0.7510    0.7381    0.7408        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_names = ['No-COVID', 'COVID']\n",
    "print(classification_report(pat_label_gt, pat_label_pr, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1934813', '1941042', '1234665', '1234660', '1234703', '1944494', '1944098', '1932991', '1948838', '1234714', '1234683', '1936916', '1929407', '1234651', '1934990', '1935297', '1952007', '1942841', '1925208', '1944643', '1933009', '1234676', '1935046', '1927330', '1925052', '1234709', '1937280', '1234644', '1944492', '1234647', '1945594', '1234650', '1234657', '1234681', '1234649', '1934915', '1935091', '1935875', '1234697', '1923960', '1935362', '1932652', '1234675', '1234678', '1939077', '1596773', '1875695', '1234606', '1765184', '1925841', '1911030', '1739046', '1910787', '1929851', '1801474', '1234636', '1724545', '1321733', '1592040', '1761939', '1919053', '1760384', '1895254', '1611801', '1598906', '1759540', '1838821', '1604132', '1604234', '1735070', '1234633', '1234586', '1768980', '1700984', '1723943', '1601335', '1923084', '1604331', '1763514', '1922987', '1906333', '1234625', '1605230', '1930107']\n"
     ]
    }
   ],
   "source": [
    "print(unique_pat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
