{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.1.0+626.g63eec3a.dirty\n",
      "Python version: 3.7.4 (default, Jul 18 2019, 19:34:02)  [GCC 5.4.0]\n",
      "Numpy version: 1.18.1\n",
      "Pytorch version: 1.5.0\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.3.0\n",
      "Nibabel version: 3.1.0\n",
      "scikit-image version: 0.14.2\n",
      "Pillow version: 7.0.0\n",
      "Tensorboard version: 2.1.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n",
      "EU toy data 4-fold segmentation for micro-challenge: fold-1\n",
      "----------\n",
      "epoch 1/150\n",
      "1/72, train_loss: 2.2126\n",
      "2/72, train_loss: 1.5635\n",
      "3/72, train_loss: 1.0396\n",
      "4/72, train_loss: 0.8888\n",
      "5/72, train_loss: 0.8839\n",
      "6/72, train_loss: 0.7266\n",
      "7/72, train_loss: 0.7356\n",
      "8/72, train_loss: 1.2346\n",
      "9/72, train_loss: 0.8741\n",
      "10/72, train_loss: 0.6929\n",
      "11/72, train_loss: 0.8086\n",
      "12/72, train_loss: 0.8144\n",
      "13/72, train_loss: 0.8665\n",
      "14/72, train_loss: 0.7738\n",
      "15/72, train_loss: 0.6920\n",
      "16/72, train_loss: 0.8503\n",
      "17/72, train_loss: 0.9560\n",
      "18/72, train_loss: 0.8795\n",
      "19/72, train_loss: 0.9284\n",
      "20/72, train_loss: 0.8775\n",
      "21/72, train_loss: 0.6822\n",
      "22/72, train_loss: 0.7134\n",
      "23/72, train_loss: 0.7050\n",
      "24/72, train_loss: 0.7060\n",
      "25/72, train_loss: 0.7385\n",
      "26/72, train_loss: 1.0898\n",
      "27/72, train_loss: 0.7892\n",
      "28/72, train_loss: 0.6352\n",
      "29/72, train_loss: 1.0002\n",
      "30/72, train_loss: 0.4631\n",
      "31/72, train_loss: 1.3397\n",
      "32/72, train_loss: 0.7850\n",
      "33/72, train_loss: 1.2598\n",
      "34/72, train_loss: 0.8117\n",
      "35/72, train_loss: 1.0932\n",
      "36/72, train_loss: 1.0675\n",
      "37/72, train_loss: 0.9777\n",
      "38/72, train_loss: 0.6268\n",
      "39/72, train_loss: 0.8149\n",
      "40/72, train_loss: 0.6060\n",
      "41/72, train_loss: 0.9168\n",
      "42/72, train_loss: 0.7283\n",
      "43/72, train_loss: 0.7316\n",
      "44/72, train_loss: 0.8774\n",
      "45/72, train_loss: 0.6260\n",
      "46/72, train_loss: 0.7340\n",
      "47/72, train_loss: 0.6442\n",
      "48/72, train_loss: 0.7717\n",
      "49/72, train_loss: 0.5961\n",
      "50/72, train_loss: 0.6350\n",
      "51/72, train_loss: 0.8150\n",
      "52/72, train_loss: 0.7556\n",
      "53/72, train_loss: 0.7743\n",
      "54/72, train_loss: 0.4598\n",
      "55/72, train_loss: 0.4504\n",
      "56/72, train_loss: 0.5704\n",
      "57/72, train_loss: 0.5454\n",
      "58/72, train_loss: 0.3194\n",
      "59/72, train_loss: 0.7826\n",
      "60/72, train_loss: 0.5695\n",
      "61/72, train_loss: 1.0123\n",
      "62/72, train_loss: 0.6518\n",
      "63/72, train_loss: 0.5475\n",
      "64/72, train_loss: 0.4938\n",
      "65/72, train_loss: 0.6560\n",
      "66/72, train_loss: 0.6511\n",
      "67/72, train_loss: 0.6401\n",
      "68/72, train_loss: 0.6362\n",
      "69/72, train_loss: 0.5995\n",
      "70/72, train_loss: 0.5469\n",
      "71/72, train_loss: 0.8104\n",
      "72/72, train_loss: 1.0542\n",
      "epoch 1 average loss: 0.8029\n",
      "saved new best metric model\n",
      "current epoch: 1 current val loss: 0.7583 best val loss: 0.7583 at epoch 1\n",
      "----------\n",
      "epoch 2/150\n",
      "1/72, train_loss: 0.7265\n",
      "2/72, train_loss: 0.5561\n",
      "3/72, train_loss: 0.6013\n",
      "4/72, train_loss: 0.7436\n",
      "5/72, train_loss: 0.7183\n",
      "6/72, train_loss: 0.5818\n",
      "7/72, train_loss: 1.0230\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-92a7f1887bff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-92a7f1887bff>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ENV/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "import glob\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.data import NiftiDataset, create_test_image_3d, list_data_collate\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import \\\n",
    "    Compose, LoadNiftid, AddChanneld, ScaleIntensityRanged, CropForegroundd, \\\n",
    "    RandCropByPosNegLabeld, RandSpatialCropd, RandAffined, Spacingd, Orientationd, ToTensord\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "\n",
    "from torch import nn\n",
    "monai.config.print_config()\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "import warnings\n",
    "\n",
    "from monai.networks import one_hot\n",
    "from monai.utils import MetricReduction\n",
    "\n",
    "\n",
    "class CrossentropyND(torch.nn.CrossEntropyLoss):\n",
    "    \"\"\"\n",
    "    Network has to have NO NONLINEARITY!\n",
    "    \"\"\"\n",
    "    def forward(self, inp, target):\n",
    "        target = target.long()\n",
    "        num_classes = inp.size()[1]\n",
    "\n",
    "        i0 = 1\n",
    "        i1 = 2\n",
    "\n",
    "        while i1 < len(inp.shape): # this is ugly but torch only allows to transpose two axes at once\n",
    "            inp = inp.transpose(i0, i1)\n",
    "            i0 += 1\n",
    "            i1 += 1\n",
    "\n",
    "        inp = inp.contiguous()\n",
    "        inp = inp.view(-1, num_classes)\n",
    "\n",
    "        target = target.view(-1,)\n",
    "\n",
    "        return super(CrossentropyND, self).forward(inp, target)\n",
    "    \n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, apply_nonlin=None, batch_dice=False, do_bg=True, smooth=1.):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "\n",
    "        self.do_bg = do_bg\n",
    "        self.batch_dice = batch_dice\n",
    "        self.apply_nonlin = apply_nonlin\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, x, y, loss_mask=None):\n",
    "        shp_x = x.shape\n",
    "\n",
    "        if self.batch_dice:\n",
    "            axes = [0] + list(range(2, len(shp_x)))\n",
    "        else:\n",
    "            axes = list(range(2, len(shp_x)))\n",
    "\n",
    "        if self.apply_nonlin is not None:\n",
    "            x = self.apply_nonlin(x)\n",
    "\n",
    "        tp, fp, fn, _ = get_tp_fp_fn_tn(x, y, axes, loss_mask, False)\n",
    "\n",
    "        nominator = 2 * tp + self.smooth\n",
    "        denominator = 2 * tp + fp + fn + self.smooth\n",
    "\n",
    "        dc = nominator / denominator\n",
    "\n",
    "        if not self.do_bg:\n",
    "            if self.batch_dice:\n",
    "                dc = dc[1:]\n",
    "            else:\n",
    "                dc = dc[:, 1:]\n",
    "        dc = dc.mean()\n",
    "\n",
    "        return -dc\n",
    "    \n",
    "def get_tp_fp_fn_tn(net_output, gt, axes=None, mask=None, square=False):\n",
    "    \"\"\"\n",
    "    net_output must be (b, c, x, y(, z)))\n",
    "    gt must be a label map (shape (b, 1, x, y(, z)) OR shape (b, x, y(, z))) or one hot encoding (b, c, x, y(, z))\n",
    "    if mask is provided it must have shape (b, 1, x, y(, z)))\n",
    "    :param net_output:\n",
    "    :param gt:\n",
    "    :param axes: can be (, ) = no summation\n",
    "    :param mask: mask must be 1 for valid pixels and 0 for invalid pixels\n",
    "    :param square: if True then fp, tp and fn will be squared before summation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        axes = tuple(range(2, len(net_output.size())))\n",
    "\n",
    "    shp_x = net_output.shape\n",
    "    shp_y = gt.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if len(shp_x) != len(shp_y):\n",
    "            gt = gt.view((shp_y[0], 1, *shp_y[1:]))\n",
    "\n",
    "        if all([i == j for i, j in zip(net_output.shape, gt.shape)]):\n",
    "            # if this is the case then gt is probably already a one hot encoding\n",
    "            y_onehot = gt\n",
    "        else:\n",
    "            gt = gt.long()\n",
    "            y_onehot = torch.zeros(shp_x)\n",
    "            if net_output.device.type == \"cuda\":\n",
    "                y_onehot = y_onehot.cuda(net_output.device.index)\n",
    "            y_onehot.scatter_(1, gt, 1)\n",
    "\n",
    "    tp = net_output * y_onehot\n",
    "    fp = net_output * (1 - y_onehot)\n",
    "    fn = (1 - net_output) * y_onehot\n",
    "    tn = (1 - net_output) * (1 - y_onehot)\n",
    "\n",
    "    if mask is not None:\n",
    "        tp = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(tp, dim=1)), dim=1)\n",
    "        fp = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(fp, dim=1)), dim=1)\n",
    "        fn = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(fn, dim=1)), dim=1)\n",
    "        tn = torch.stack(tuple(x_i * mask[:, 0] for x_i in torch.unbind(tn, dim=1)), dim=1)\n",
    "\n",
    "    if square:\n",
    "        tp = tp ** 2\n",
    "        fp = fp ** 2\n",
    "        fn = fn ** 2\n",
    "        tn = tn ** 2\n",
    "\n",
    "    if len(axes) > 0:\n",
    "        tp = sum_tensor(tp, axes, keepdim=False)\n",
    "        fp = sum_tensor(fp, axes, keepdim=False)\n",
    "        fn = sum_tensor(fn, axes, keepdim=False)\n",
    "        tn = sum_tensor(tn, axes, keepdim=False)\n",
    "\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "class DC_and_CE_loss(nn.Module):\n",
    "    def __init__(self, soft_dice_kwargs, ce_kwargs, aggregate=\"sum\", square_dice=False, weight_ce=1, weight_dice=1):\n",
    "        \"\"\"\n",
    "        CAREFUL. Weights for CE and Dice do not need to sum to one. You can set whatever you want.\n",
    "        :param soft_dice_kwargs:\n",
    "        :param ce_kwargs:\n",
    "        :param aggregate:\n",
    "        :param square_dice:\n",
    "        :param weight_ce:\n",
    "        :param weight_dice:\n",
    "        \"\"\"\n",
    "        super(DC_and_CE_loss, self).__init__()\n",
    "        self.weight_dice = weight_dice\n",
    "        self.weight_ce = weight_ce\n",
    "        self.aggregate = aggregate\n",
    "        self.ce = CrossentropyND(**ce_kwargs)\n",
    "        self.dc = SoftDiceLoss(apply_nonlin=softmax_helper, **soft_dice_kwargs)\n",
    "\n",
    "    def forward(self, net_output, target):\n",
    "        dc_loss = self.dc(net_output, target) if self.weight_dice != 0 else 0\n",
    "        ce_loss = self.ce(net_output, target) if self.weight_ce != 0 else 0\n",
    "        if self.aggregate == \"sum\":\n",
    "            result = self.weight_ce * ce_loss + self.weight_dice * dc_loss\n",
    "        else:\n",
    "            raise NotImplementedError(\"nah son\") # reserved for other stuff (later)\n",
    "        return result\n",
    "\n",
    "def softmax_helper(x):\n",
    "    rpt = [1 for _ in range(len(x.size()))]\n",
    "    rpt[1] = x.size(1)\n",
    "    x_max = x.max(1, keepdim=True)[0].repeat(*rpt)\n",
    "    e_x = torch.exp(x - x_max)\n",
    "    return e_x / e_x.sum(1, keepdim=True).repeat(*rpt)\n",
    "\n",
    "def sum_tensor(inp, axes, keepdim=False):\n",
    "    axes = np.unique(axes).astype(int)\n",
    "    if keepdim:\n",
    "        for ax in axes:\n",
    "            inp = inp.sum(int(ax), keepdim=True)\n",
    "    else:\n",
    "        for ax in sorted(axes, reverse=True):\n",
    "            inp = inp.sum(int(ax))\n",
    "    return inp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    \n",
    "    print('EU toy data 4-fold segmentation for micro-challenge: fold-1')\n",
    "\n",
    "    # Supervised learning data for training and validation\n",
    "    data_dir = '/home/marafath/scratch/eu_challenge/synthetic_data'\n",
    "    train_images = sorted(glob.glob(os.path.join(data_dir, '*_vol.nii.gz')))\n",
    "    train_labels = sorted(glob.glob(os.path.join(data_dir, '*_labels.nii.gz')))\n",
    "    data_dicts = [{'image': image_name, 'label': label_name}\n",
    "                  for image_name, label_name in zip(train_images, train_labels)]\n",
    "\n",
    "    fold = 1\n",
    "    epc = 150\n",
    "    train_files, val_files = data_dicts[0:72], data_dicts[72:96]\n",
    "        \n",
    "    '''\n",
    "    if i == 0:\n",
    "        train_files, val_files = data_dicts[0:72], data_dicts[72:96]\n",
    "    elif i == 1:\n",
    "        train_files = data_dicts[0:48] + data_dicts[72:96]\n",
    "        val_files = data_dicts[48:72]\n",
    "    elif i == 2:\n",
    "        train_files = data_dicts[0:24] + data_dicts[48:96]\n",
    "        val_files = data_dicts[24:48]\n",
    "    elif i == 3:\n",
    "        train_files, val_files = data_dicts[24:96], data_dicts[0:24]\n",
    "    '''\n",
    "\n",
    "    # Defining Transform\n",
    "    train_transforms = Compose([\n",
    "        LoadNiftid(keys=['image', 'label']),\n",
    "        AddChanneld(keys=['image', 'label']),\n",
    "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 1.5), mode=('bilinear', 'nearest')),\n",
    "        Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "        ScaleIntensityRanged(keys=['image'], a_min=-1250, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "        RandCropByPosNegLabeld(keys=['image', 'label'], label_key='label', spatial_size=(96, 96, 96), pos=1,\n",
    "                               neg=1, num_samples=4, image_key='image', image_threshold=0),\n",
    "        ToTensord(keys=['image', 'label'])\n",
    "    ])\n",
    "    val_transforms = Compose([\n",
    "        LoadNiftid(keys=['image', 'label']),\n",
    "        AddChanneld(keys=['image', 'label']),\n",
    "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 1.5), mode=('bilinear', 'nearest')),\n",
    "        Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "        ScaleIntensityRanged(keys=['image'], a_min=-1250, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "        RandCropByPosNegLabeld(keys=['image', 'label'], label_key='label', spatial_size=(96, 96, 96), pos=1,\n",
    "                               neg=1, num_samples=4, image_key='image', image_threshold=0),\n",
    "        ToTensord(keys=['image', 'label'])\n",
    "    ])\n",
    "\n",
    "    # Data loader\n",
    "    train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "    train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            collate_fn=list_data_collate,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
    "    val_loader = DataLoader(\n",
    "            val_ds, \n",
    "            batch_size=1, \n",
    "            num_workers=4, \n",
    "            collate_fn=list_data_collate,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "\n",
    "    # Defining model and hyperparameters\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = monai.networks.nets.UNet(\n",
    "        dimensions=3,\n",
    "        in_channels=1,\n",
    "        out_channels=7,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        strides=(2, 2, 2, 2),\n",
    "        num_res_units=2\n",
    "    ).to(device)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    loss_function = DC_and_CE_loss({'smooth': 1e-5, 'do_bg': False}, {})\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-2)\n",
    "\n",
    "    # start a typical PyTorch training\n",
    "    val_interval = 1\n",
    "    best_metric = 1e10\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = list()\n",
    "    metric_values = list()\n",
    "    writer = SummaryWriter()\n",
    "    for epoch in range(epc):\n",
    "        print('-' * 10)\n",
    "        print(f\"epoch {epoch + 1}/{epc}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = batch_data['image'].to(device), batch_data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print(f\"{step}/{len(train_ds) // train_loader.batch_size}, train_loss: {loss.item():.4f}\")\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                metric_sum = 0.0\n",
    "                metric_count = 0\n",
    "                val_images = None\n",
    "                val_labels = None\n",
    "                val_outputs = None\n",
    "                for val_data in val_loader:\n",
    "                    val_images, val_labels = val_data[\"image\"].to(device), val_data[\"label\"].to(device)\n",
    "                    outputs_ = model(val_images)\n",
    "                    loss_ = loss_function(outputs_, val_labels)\n",
    "                    metric_sum += loss_.item()\n",
    "                metric = metric_sum / len(val_ds)\n",
    "                metric_values.append(metric)\n",
    "                if metric < best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), '/home/marafath/scratch/saved_models/eu_best_f{}.pth'.format(fold))\n",
    "                    print(\"saved new best metric model\")\n",
    "                print(\n",
    "                    \"current epoch: {} current val loss: {:.4f} best val loss: {:.4f} at epoch {}\".format(\n",
    "                        epoch + 1, metric, best_metric, best_metric_epoch\n",
    "                    )\n",
    "                )\n",
    "                writer.add_scalar(\"val_mean_loss\", metric, epoch + 1)\n",
    "\n",
    "    print('train completed')\n",
    "    writer.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
